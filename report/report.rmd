---
title: "Comparing Hamiltonian Monte Carlo and Elliptical Slice Sampling for constrained Gaussian distributions"
subtitle: "732A76 Research Project Report"
author: "Bayu Brahmantio (baybr878)"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
        pdf_document:
                extra_dependencies: "amsmath"
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 8)
library(knitr)
library(stats)
```

# 1. Background  

High-dimensional multivariate gaussian distribution is used in various models and applications. In some cases, we need to generate from a certain distribution which applies constraints to a multivariate gaussian distribution (Gelfand et al. (1992) and Rodriguez-Yam et al. (2004)). Sampling from this distribution is still a challenging issue, particularly because it is not straightforward to compute the normalizing constant for the density function.  

The gibbs sampler has proven to be a suitable choices to sample from truncated mutivariate gaussian distributions (Gelfand et al. (1992)). Recently, more sophisticated methods have been developed to generate samples from truncated mutivariate gaussian distributions. In this research project, two methods, namely Exact Hamiltonian Monte Carlo (Pakman et al. (2014)) and Analytic Elliptical Slice Sampling (Fagan et al. (2016)), will be compared.   


# 2. Definitions    

## 2.1. Truncated Multivariate Gaussian Distribution 


The truncated multivariate gaussian distribution is a probability distribution obtained from multivariate gaussian random variable by bounding it under some linear (or quadratic) constraints.   

Let $\textrm{W}$ be a $d$-dimensional gaussian random variable with mean vector $mu$ and covariance matrix $\Sigma$. The corresponding truncated multivariate gaussian distribution can be defined as:   

$$f_{\textrm{X}}(\textrm{x})=
\frac{\exp\{-\frac{1}{2}(\textrm{x}-\mu)^T\Sigma^{-1}(\textrm{x}-\mu)\}}
{\int_{a}^{b}\exp\{-\frac{1}{2}(\textrm{x}-\mu)^T\Sigma^{-1}(\textrm{x}-\mu)\}d\textrm{x}}I(a\leq\textrm{x}\leq b)$$

Where $X$ is is a $d$-dimensional truncated gaussian random variable and *$a$* and *$b$* are the lower and upper bounds for $X$ respectively.  

We can rewrite $X$ as:   

$$\log f_{\textrm{X}}(\textrm{x}) = -\frac{1}{2}\textrm{x}^T \Lambda \textrm{x} + \nu^T\textrm{x} + const.$$   
where $\Lambda = \Sigma^{-1}$ and $\nu = \Sigma^{-1}\mu$. Let $\textrm{F}$ be an $m \times d$ matrix and let $g$ be an $m \times 1$ vector. Then, $\textrm{X}$ must satisfy   

$$\textrm{F}_j\textrm{x} + g_j \geq 0, \quad j=1,..., m$$

## 2.2. Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians   

Exact Hamiltonian Monte Carlo (HMC) for Truncated Multivariate Gaussians (TMG) (Pakman et al. (2014)) considers the exact paths of sample trajectories that follow the Hamiltonian equations of motion. It takes the Hamiltonian function     

$$H = \frac{1}{2}\textrm{x}^T\textrm{x} + \frac{1}{2}\textrm{s}^T\textrm{s}$$
in which the joint distribution is $P(\textrm{X}, \textrm{S}) = \exp(-H)$. With this form of $H$, we can derive the Hamilton's equations as   

$$\begin{aligned}
\frac{\partial x_i}{\partial t} & = \frac{\partial H}{\partial s_i}=s_i \\
\frac{\partial s_i}{\partial t} & = -\frac{\partial H}{\partial x_i}=-x_i \\
\end{aligned}$$

for $i=1,...,d$. The solution to those equations is   

$$x_i(t) = s_i(0)\sin(t) + x_i(0)\cos(t)$$
which we can use as the trajectory of the particle along the state space.   

To sample $\textrm{X}$, we first sample values of $\textrm{S}$ from $\mathcal{N}(\textbf{0},\textrm{I}_d)$ and use them with previous values, $\textrm{x}$, as the initial conditions. We will then move along this trajectory for a time $T$ until we reach a new point $\textrm{x}^*$. This sequence forms a markov chain that converges to the distribution $P(\textrm{X})$.   


## 2.3. Analytic Elliptical Slice Sampling   

Elliptical Slice Sampling (ESS, Murray et al. (2010)) aims to sample from posterior of the form   

$$P^*(\textrm{X}) = \frac1Z \mathcal{N}(\textrm{X};\textbf{0},\Sigma)L(\textrm{X})$$
where $Z$ is the normalization constant, $\mathcal{N}(\textbf{0},\Sigma)$ is a multivariate gaussian prior, and $L$ is a likelihood function. The algorithm begins by sampling values of $\boldsymbol{\nu}$ which, together with the values of current state $\textbf{x}$, make an ellipse of possible new values   
$$\textbf{x}' = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)$$.

We then sample $y$ uniformly between $[0, L(\textbf{x})]$ to determine the height of the slice. To sample a new state from the ellipse, we sample values from the bracket $[0, 2\pi)$. A proposed point is accepted if it satisfies $L(\textbf{x}') > y$. Otherwise, we shrink the bracket towards $\theta = 0$ until the proposed state is accepted.   

Analytic Elliptical Slice Sampling (Fagan et al. (2016)) is an extension to ESS where the possible sampling bracket is pre-determined. To do this, we define   

$$\mathcal{E} = \{\textbf{x}':\textbf{x}'(\theta) = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)\}$$
as the set of possible new states and define   

$$\mathcal{S}(y,\mathcal{E}) = \{ \textbf{x}'\in \mathcal{E}: L(\textbf{x}')>y\}$$  
as the slice of acceptable proposed states. To form a markov chain that converges to the posterior distribution, we need to sample uniformly from the analytically pre-determined $\mathcal{S}(y,\mathcal{E})$.   


# 3. Results   



Using example in Figure 1 in [Pakman, 2012].

**Trace plots:**   
```{r, eval=FALSE}
# plots
Xs_ESS = readRDS("x_ESS.rds")
Xs_HMC = readRDS("x_EHMC.rds")

par(mfrow=c(2,2))
plot(Xs_ESS[1,], Xs_ESS[2,], cex = 0.5,
     xlab = "X1", ylab = "X2", col = "blue",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "Elliptical Slice Sampling")
abline(a=0, b=1)
abline(a=0, b=1.1)

plot(Xs_HMC[1,], Xs_HMC[2,], cex = 0.5,
     xlab = "X1", ylab = "X2", col = "red",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "Exact HMC")
abline(a=0, b=1)
abline(a=0, b=1.1)

# trace plots of first 400 iterations of X2
plot(Xs_ESS[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "X2", col = "blue",
     xlim = c(1,400), ylim = c(0,8))

plot(Xs_HMC[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "X2", col = "red",
     xlim = c(1,400), ylim = c(0,8))

```

\newpage

**ACF plots:**  
```{r, eval=FALSE}
# acf plots
acf_ESS1 = acf(Xs_ESS[1,], lag.max = 100, plot = F)
acf_ESS2 = acf(Xs_ESS[2,], lag.max = 100, plot = F)

acf_HMC1 = acf(Xs_HMC[1,], lag.max = 100, plot = F)
acf_HMC2 = acf(Xs_HMC[2,], lag.max = 100, plot = F)

par(mfrow=c(2,2))
plot(acf_ESS1, type = "l", main = "X1 (ESS)")
plot(acf_HMC1, type = "l", main = "X1 (HMC)")
plot(acf_ESS2, type = "l", main = "X2 (ESS)")
plot(acf_HMC2, type = "l", main = "X2 (HMC)")


```

**Computation time / iteration (s):**   
```{r, eval=FALSE}
t_ESS = readRDS("t_ESS.rds")
t_HMC = readRDS("t_EHMC.rds")

means = c(mean(t_ESS), mean(t_HMC))
M = matrix(means, nrow = 1)
colnames(M) = c("ESS", "HMC")

kable(M)
```


\newpage