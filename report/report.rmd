---
title: "Comparing Hamiltonian Monte Carlo and Elliptical Slice Sampling for constrained Gaussian distributions"
subtitle: "732A76 Research Project Report"
author: "Bayu Brahmantio (baybr878)"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
        pdf_document:
                extra_dependencies: "amsmath"
bibliography: ref.bib
nocite: '@*'
csl: american-statistical-association.csl
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
library(knitr)
library(stats)
library(bookdown)
library(tinytex)
```

# 1. Background  

High-dimensional multivariate gaussian distribution is used in various models and applications. In some cases, we need to generate from a certain distribution which applies constraints to a multivariate gaussian distribution (Gelfand et al. (1992) and Rodriguez-Yam et al. (2004)). Sampling from this distribution is still a challenging issue, particularly because it is not straightforward to compute the normalizing constant for the density function.  

The gibbs sampler has proven to be a suitable choices to sample from truncated mutivariate gaussian distributions (Gelfand et al. (1992)). Recently, more sophisticated methods have been developed to generate samples from truncated mutivariate gaussian distributions. In this research project, two methods, namely Exact Hamiltonian Monte Carlo (Pakman et al. (2013)) and Analytic Elliptical Slice Sampling (Fagan et al. (2016)), will be compared.   


# 2. Definitions    

## 2.1. Truncated Multivariate Gaussian Distribution 


The truncated multivariate gaussian distribution is a probability distribution obtained from multivariate gaussian random variable by bounding it under some linear (or quadratic) constraints.   

Let $\textbf{w}$ be a $d$-dimensional gaussian random variable with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$. The corresponding truncated multivariate gaussian distribution can be defined as:   


$$
{p(\boldsymbol{\textbf{x}})=\frac{\exp\{-\frac{1}{2}(\textbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\textbf{x}-\boldsymbol{\mu})\}}{\int_{\textbf{F}\textbf{x} + g \geq 0}\exp\{-\frac{1}{2}(\textbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\textbf{x}-\boldsymbol{\mu})\}d\textbf{x}}I(\textbf{F}\textbf{x} + g \geq 0)}
$$


Where $\textbf{x}$ is a $d$-dimensional truncated gaussian random variable, $I$ is an indicator function, and $\textbf{F}$ is an $m \times d$ matrix, which, together with the $m \times 1$ vector of $\textbf{g}$, defines all $m$ constraints of $p(\boldsymbol{\textbf{x}})$.  We denote this as $\textbf{x} \sim TN(\boldsymbol{\mu}, \boldsymbol{\Sigma};\textbf{F},\textbf{g})$. We can rewrite $p(\boldsymbol{\textbf{x}})$ as:   

$$
p(\textbf{x}) = \frac1Z\exp\bigg\{-\frac{1}{2}\textbf{x}^T \boldsymbol{\Lambda} \textbf{x} + \boldsymbol{\nu}^T\textbf{x}\bigg\}I(\textbf{F}\textbf{x} + \textbf{g} \geq 0)
$$

where $\boldsymbol{\Lambda} = \boldsymbol{\Sigma}^{-1}$, $\boldsymbol{\nu} = \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}$, and $Z$ is the normalizing constant. Through linear change of variables, ($\ref{eq:tmvg2}$) can be transformed into:   

\begin{equation}
$$
p(\textbf{x}) = \frac1Z\exp\bigg\{-\frac{1}{2}\textbf{x}^T \textbf{x}\bigg\}I(\textbf{F}^*\textbf{x} + \textbf{g}^* \geq 0)
$$
\label{eq:tmvg3}
\end{equation}

such that $\textbf{x} \sim TN(\textbf{0}, \textbf{I}_d;\textbf{F}^*,\textbf{g}^*)$.   

## 2.2. Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians   

Exact Hamiltonian Monte Carlo (HMC) for Truncated Multivariate Gaussians (TMG) (Pakman et al. (2013)) considers the exact paths of particle trajectories in a Hamiltonian system:   

\begin{equation}
$$
H(\textbf{x}, \textbf{s}) = U(\textbf{x}) + K(\textbf{s})
$$
\label{eq:hml}
\end{equation}

where $U(\textbf{x})$ is the potential energy term as a function of position ($\textbf{x}$) and $K(\textbf{s})$ is the kinetic energy term as a function of momentum ($\textbf{s}$). The change of position and momentum over time, $t$, can be described by Hamilton's equations:   

$$
\begin{align}
\frac{\partial x_i}{\partial t} & = \frac{\partial H}{\partial s_i} \\
\frac{\partial s_i}{\partial t} & = -\frac{\partial H}{\partial x_i} \qquad i=1,...,d.\\
\end{align}
$$

The target distribution is related to the energy function through canonical distribution:   

$$
p(\textbf{x}) \propto \exp\{-E(\textbf{x})\}
$$

where $p(\textbf{x})$ depends on the value of $E(\textbf{x})$. In a Hamiltonian system, we have $H(\textbf{x}, \textbf{s})$ as our energy function, which results in the canonical distribution:   

$$
p(\textbf{x}, \textbf{s}) \propto \exp\{-H(\textbf{x}, \textbf{s})\} \propto \exp\{-U(\textbf{x})\} \exp\{-K(\textbf{s})\} \propto p(\textbf{x})p(\textbf{s}).
$$
Hence, $\textbf{x}$ and $\textbf{s}$ are independent. To sample from the target distribution $p(\textbf{x})$, we can sample from the joint distribution $p(\textbf{x}, \textbf{s})$ and ignore the variable $\textbf{s}$.   


Suppose our target distribution $p(\textbf{x})$ is a truncated multivariate gaussian distribution as in ($\ref{eq:tmvg3}$). We can set our momenta to be normally distributed, that is $\textbf{s} \sim \mathcal{N}(\textbf{0}, \textbf{I}_d)$. The Hamiltonian system can be described as:   

$$H = U(\textbf{x}) + K(\textbf{s}) = \frac{1}{2}\textbf{x}^T\textbf{x} + \frac{1}{2}\textbf{s}^T\textbf{s}$$
subject to:   

$$\textbf{F}\textbf{x} + \textbf{g} \geq 0.$$
for some appropriate values of $\textbf{F}$ and $\textbf{g}$.

The equations of motion for this Hamiltonian system are:  

$$
\begin{split}
\frac{\partial x_i}{\partial t} & = \frac{\partial H}{\partial s_i} = s_i \\
\frac{\partial s_i}{\partial t} & = -\frac{\partial H}{\partial x_i} -x_i \qquad i=1,...,d\\
\end{split}
$$

In this sense, we want the particles in the Hamiltonian system to only move around inside the constrained space. Hence, it may hit walls and bounce.  

The exact trajectory of a particle using the equations above is:   

$$x_i(t) = s_i(0)\sin(t) + x_i(0)\cos(t).$$
A particle will follow the trajectory above until it hits a wall, or in other words, until $\textbf{F}\textbf{x} + \textbf{g} = 0$.    









## 2.3. Analytic Elliptical Slice Sampling   

Elliptical Slice Sampling (ESS, Murray et al. (2010)) aims to sample from posterior of the form   

$$P^*(\textbf{X}) = \frac1Z \mathcal{N}(\textbf{X};\textbf{0},\boldsymbol{\Sigma})L(\textbf{X})$$
where $Z$ is the normalization constant, $\mathcal{N}(\textbf{0},\boldsymbol{\Sigma})$ is a multivariate gaussian prior, and $L$ is a likelihood function. The algorithm begins by sampling values of $\boldsymbol{\nu}$ which, together with the values of current state $\textbf{x}$, make an ellipse of possible new values   
$$\textbf{x}' = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)$$.

We then sample $y$ uniformly between $[0, L(\textbf{x})]$ to determine the height of the slice. To sample a new state from the ellipse, we sample values from the bracket $[0, 2\pi)$. A proposed point is accepted if it satisfies $L(\textbf{x}') > y$. Otherwise, we shrink the bracket towards $\theta = 0$ until the proposed state is accepted.   

Analytic Elliptical Slice Sampling (Fagan et al. (2016)) is an extension to ESS where the possible sampling bracket is pre-determined. To do this, we define   

$$\mathcal{E} = \{\textbf{x}':\textbf{x}'(\theta) = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)\}$$
as the set of possible new states and define   

$$\mathcal{S}(y,\mathcal{E}) = \{ \textbf{x}'\in \mathcal{E}: L(\textbf{x}')>y\}$$  
as the slice of acceptable proposed states. To form a markov chain that converges to the posterior distribution, we need to sample uniformly from the analytically pre-determined $\mathcal{S}(y,\mathcal{E})$ (Fagan et al. (2016)).    


# 3. Results    

In this section we are going to compare both methods (Exact HMC for TMG and Analytic ESS) under different settings of truncated multivariate gaussians. Fot both of them, we use linear change of variables to transform the distribution into a whitened frame such that the generated points are from $\mathcal{N}(\textbf{0},\textbf{I}_d)$.  

Figure 1 shows how both algorithms sample a two-dimensional gaussian distribution with some constraints that results in a narrow support, similar to Figure 1 in Pakman et al. (2013). From both cases, we can see that they worked as expected. The generated samples rapidly converged to the mean while also complying the constraints. The autocorrelation function plots also show that the samples are not highly correlated. However,the Exact HMC tends to be spread more across the support than Analytic ESS.   

 
```{r,fig.cap="Comparison of generated points from $\\mathcal{N}((4,4)^T,\\textbf{I}_2)$ under the constraints $X_1 \\leq X_2 \\leq1.1 X_1$ and $X_1,X_2 \\geq 0$ with initial points (2, 2.1). First column: 1000 iterations. Second column: trace plots of the first 400 iterations of $X_2$. Third column: Autocorrelation of $X_2$. In this example we used $T=pi/2$ for HMC and $J=1$ for ESS. Both methods converges around $X_2=4$ and have relatively low autocorrelation scores but HMC appears to explore the possible space more."}
# plots
Xs_ESS = readRDS("x_EESS_J_1.rds")
Xs_HMC = readRDS("x_EHMC.rds")

acf_ESS2 = acf(Xs_ESS[2,], lag.max = 100, plot = F)
acf_HMC2 = acf(Xs_HMC[2,], lag.max = 100, plot = F)

# ESS
par(mfrow=c(2,3), oma = c(0,4,0,0))

plot(Xs_ESS[1,], Xs_ESS[2,], cex = 0.5,
     xlab = "x1", 
     ylab = "x2", 
     col = "blue",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "")
mtext(side=2, line=5, "Analytic ESS", font=2, cex=1.2)
abline(a=0, b=1)
abline(a=0, b=1.1)

plot(Xs_ESS[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "x2", col = "blue",
     xlim = c(1,400), ylim = c(0,8))

plot(acf_ESS2, type = "l", main = "")

# HMC
plot(Xs_HMC[1,], Xs_HMC[2,], cex = 0.5,
     xlab = "x1", 
     ylab = "x2", 
     col = "red",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "")
mtext(side=2, line=5, "Exact HMC", font=2, cex=1.2)
abline(a=0, b=1)
abline(a=0, b=1.1)

plot(Xs_HMC[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "x2", col = "red",
     xlim = c(1,400), ylim = c(0,8))

plot(acf_HMC2, type = "l", main = "")

```

Notice that the maximum traveling time $(T)$ and number of samples per iteration $(J)$ affect the computation time for Exact HMC and Analytic ESS respectively. The higher the maximum traveling time ($T$), the longer it takes for a particle in an HMC system to move while also increases the possibility of bouncing. At the same time, an increase in the value of $J$ lets us sample multiple points from the same ellipse without a significant increase in computation time.  

From the experiments, we observed a large disparity between the runtimes of both algorithms. As seen in the Table 1, Exact HMC performs much faster than the fastest Analytic ESS.


```{r}
# Computation time / iteration (s):
t_HMC_pi2 = readRDS("t_EHMC_HD_pi2.rds")
t_HMC_3pi4 = readRDS("t_EHMC_HD_3pi4.rds")

t_ESS_1 = readRDS("t_EESS_HD_J1.rds")
t_ESS_5 = readRDS("t_EESS_HD_J5.rds")
t_ESS_10 = readRDS("t_EESS_HD_J10.rds")

means = c(mean(t_HMC_pi2), mean(t_HMC_3pi4), mean(t_ESS_1), mean(t_ESS_5)/5, mean(t_ESS_10)/10)
M = matrix(means, ncol = 1)
M = cbind(c("HMC(T=pi/2)", "HMC(T=3*pi/4)", 
                "ESS(J=1)", "ESS(J=5)", "ESS(J=10)"),
          round(M, 7))
colnames(M) = c("Model", "Avg. computation time per sample (second)")

kable(M,
      caption = "Table 1: Average computation time per sample for both models under different settings for 10-dimensional TMG under the constraints $-1 \\leq x_i \\leq 1, i=1,...,10$ using CPU. For HMC, T is the maximum travel time. For ESS, J is the number of points sampled in each iteration. The computation time for Exact HMC is overall much faster than Analytic ESS.")



```

# 4. Discussion    

In this research project, we have compared Exact HMC and Analytic ESS. In terms of the generated samples, they are not significantly different. Exact HMC also performs much faster than Analytic ESS. More rigorous measurements can be made to evaluate whether the samples are autocorrelated within a chain or not by measuring their effective sample size.     

There are, however, some caveats to the computation of Analytic ESS in this case. We expected the runtime to be somewhat similar to Exact HMC, but it runs more than 100 times slower. This could be to the fact that external packages are used for the computation of the intervals and slices which could have been replaced by customized functions that are more specific to the task, hence the faster runtimes. The programming language used (`R`) is also not popularly chosen because of its speed. Thus, the algorithm might need to be rewritten in a language that is proven to be computationally faster.   



# 5. References   

<div id="refs"></div>


# 6. Appendix  

All codes are available on `https://github.com/bayubeta/research-project`. 