---
title: "Comparing Hamiltonian Monte Carlo and Elliptical Slice Sampling for constrained Gaussian distributions"
subtitle: "732A76 Research Project Report"
author: "Bayu Brahmantio (baybr878)"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
        pdf_document:
                extra_dependencies: "amsmath"
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 4)
library(knitr)
library(stats)
```

# 1. Background  

High-dimensional multivariate gaussian distribution is used in various models and applications. In some cases, we need to generate from a certain distribution which applies constraints to a multivariate gaussian distribution (Gelfand et al. (1992) and Rodriguez-Yam et al. (2004)). Sampling from this distribution is still a challenging issue, particularly because it is not straightforward to compute the normalizing constant for the density function.  

The gibbs sampler has proven to be a suitable choices to sample from truncated mutivariate gaussian distributions (Gelfand et al. (1992)). Recently, more sophisticated methods have been developed to generate samples from truncated mutivariate gaussian distributions. In this research project, two methods, namely Exact Hamiltonian Monte Carlo (Pakman et al. (2014)) and Analytic Elliptical Slice Sampling (Fagan et al. (2016)), will be compared.   


# 2. Definitions    

## 2.1. Truncated Multivariate Gaussian Distribution 


The truncated multivariate gaussian distribution is a probability distribution obtained from multivariate gaussian random variable by bounding it under some linear (or quadratic) constraints.   

Let $\textrm{W}$ be a $d$-dimensional gaussian random variable with mean vector $mu$ and covariance matrix $\Sigma$. The corresponding truncated multivariate gaussian distribution can be defined as:   

$$f_{\textrm{X}}(\textrm{x})=
\frac{\exp\{-\frac{1}{2}(\textrm{x}-\mu)^T\Sigma^{-1}(\textrm{x}-\mu)\}}
{\int_{a}^{b}\exp\{-\frac{1}{2}(\textrm{x}-\mu)^T\Sigma^{-1}(\textrm{x}-\mu)\}d\textrm{x}}I(a\leq\textrm{x}\leq b)$$

Where $X$ is is a $d$-dimensional truncated gaussian random variable and *$a$* and *$b$* are the lower and upper bounds for $X$ respectively.  

We can rewrite $X$ as:   

$$\log f_{\textrm{X}}(\textrm{x}) = -\frac{1}{2}\textrm{x}^T \Lambda \textrm{x} + \nu^T\textrm{x} + const.$$   
where $\Lambda = \Sigma^{-1}$ and $\nu = \Sigma^{-1}\mu$. Let $\textrm{F}$ be an $m \times d$ matrix and let $g$ be an $m \times 1$ vector. Then, $\textrm{X}$ must satisfy   

$$\textrm{F}_j\textrm{x} + g_j \geq 0, \quad j=1,..., m$$

## 2.2. Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians   

Exact Hamiltonian Monte Carlo (HMC) for Truncated Multivariate Gaussians (TMG) (Pakman et al. (2014)) considers the exact paths of sample trajectories that follow the Hamiltonian equations of motion. It takes the Hamiltonian function     

$$H = \frac{1}{2}\textrm{x}^T\textrm{x} + \frac{1}{2}\textrm{s}^T\textrm{s}$$
in which the joint distribution is $P(\textrm{X}, \textrm{S}) = \exp(-H)$. With this form of $H$, we can derive the Hamilton's equations as   

$$\begin{aligned}
\frac{\partial x_i}{\partial t} & = \frac{\partial H}{\partial s_i}=s_i \\
\frac{\partial s_i}{\partial t} & = -\frac{\partial H}{\partial x_i}=-x_i \\
\end{aligned}$$

for $i=1,...,d$. The solution to those equations is   

$$x_i(t) = s_i(0)\sin(t) + x_i(0)\cos(t)$$
which we can use as the trajectory of the particle along the state space.   

To sample $\textrm{X}$, we first sample values of $\textrm{S}$ from $\mathcal{N}(\textbf{0},\textrm{I}_d)$ and use them with previous values, $\textrm{x}$, as the initial conditions. We will then move along this trajectory for a time $T$ until we reach a new point $\textrm{x}^*$. This sequence forms a markov chain that converges to the distribution $P(\textrm{X})$ (Pakman et al. (2014)).   


## 2.3. Analytic Elliptical Slice Sampling   

Elliptical Slice Sampling (ESS, Murray et al. (2010)) aims to sample from posterior of the form   

$$P^*(\textrm{X}) = \frac1Z \mathcal{N}(\textrm{X};\textbf{0},\Sigma)L(\textrm{X})$$
where $Z$ is the normalization constant, $\mathcal{N}(\textbf{0},\Sigma)$ is a multivariate gaussian prior, and $L$ is a likelihood function. The algorithm begins by sampling values of $\boldsymbol{\nu}$ which, together with the values of current state $\textbf{x}$, make an ellipse of possible new values   
$$\textbf{x}' = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)$$.

We then sample $y$ uniformly between $[0, L(\textbf{x})]$ to determine the height of the slice. To sample a new state from the ellipse, we sample values from the bracket $[0, 2\pi)$. A proposed point is accepted if it satisfies $L(\textbf{x}') > y$. Otherwise, we shrink the bracket towards $\theta = 0$ until the proposed state is accepted.   

Analytic Elliptical Slice Sampling (Fagan et al. (2016)) is an extension to ESS where the possible sampling bracket is pre-determined. To do this, we define   

$$\mathcal{E} = \{\textbf{x}':\textbf{x}'(\theta) = \textbf{x}\cos(\theta) + \boldsymbol{\nu}\sin(\theta)\}$$
as the set of possible new states and define   

$$\mathcal{S}(y,\mathcal{E}) = \{ \textbf{x}'\in \mathcal{E}: L(\textbf{x}')>y\}$$  
as the slice of acceptable proposed states. To form a markov chain that converges to the posterior distribution, we need to sample uniformly from the analytically pre-determined $\mathcal{S}(y,\mathcal{E})$ (Fagan et al. (2016)).    


# 3. Results    

In this section we are going to compare both methods (Exact HMC for TMG and Analytic ESS) under different settings of truncated multivariate gaussians. Fot both of them, we use linear change of variables to transform the distribution into a whitened frame such that the generated points are from $\mathcal{N}(\textbf{0},\textrm{I}_d)$.  

Figure 1 shows how both algorithms sample a two-dimensional gaussian distribution with some constraints that results in a narrow support, similar to Figure 1 in Pakman et al. (2014). From both cases, we can see that they worked as expected. The generated samples rapidly converged to the mean while also complying the constraints. The autocorrelation function plots also show that the samples are not highly correlated. However,the Exact HMC tends to be spread more across the support than Analytic ESS.   

 
```{r,fig.cap="Figure 1: Comparison of generated points from $\\mathcal{N}((4,4)^T,\\textrm{I}_2)$ under the constraints $X_1 \\leq X_2 \\leq1.1 X_1$ and $X_1,X_2 \\geq 0$ with initial points (2, 2.1). First column: 1000 iterations. Second column: trace plots of the first 400 iterations of $X_2$. Third column: Autocorrelation of $X_2$. In this example we used $T=pi/2$ for HMC and $J=1$ for ESS. Both methods converges around $X_2=4$ and have relatively low autocorrelation scores but HMC appears to explore the possible space more."}
# plots
Xs_ESS = readRDS("x_EESS_J_1.rds")
Xs_HMC = readRDS("x_EHMC.rds")

acf_ESS2 = acf(Xs_ESS[2,], lag.max = 100, plot = F)
acf_HMC2 = acf(Xs_HMC[2,], lag.max = 100, plot = F)

# ESS
par(mfrow=c(2,3), oma = c(0,4,0,0))

plot(Xs_ESS[1,], Xs_ESS[2,], cex = 0.5,
     xlab = "x1", 
     ylab = "x2", 
     col = "blue",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "")
mtext(side=2, line=5, "Analytic ESS", font=2, cex=1.2)
abline(a=0, b=1)
abline(a=0, b=1.1)

plot(Xs_ESS[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "x2", col = "blue",
     xlim = c(1,400), ylim = c(0,8))

plot(acf_ESS2, type = "l", main = "")

# HMC
plot(Xs_HMC[1,], Xs_HMC[2,], cex = 0.5,
     xlab = "x1", 
     ylab = "x2", 
     col = "red",
     xlim = c(1.5,6.5), ylim = c(1.5,6.5),
     main = "")
mtext(side=2, line=5, "Exact HMC", font=2, cex=1.2)
abline(a=0, b=1)
abline(a=0, b=1.1)

plot(Xs_HMC[2,1:400], cex = 0.5, type = "l",
     xlab = "Iteration", ylab = "x2", col = "red",
     xlim = c(1,400), ylim = c(0,8))

plot(acf_HMC2, type = "l", main = "")

```

Notice that the maximum traveling time $(T)$ and number of samples per iteration $(J)$ affect the computation time for Exact HMC and Analytic ESS respectively. The higher the maximum traveling time ($T$), the longer it takes for a particle in an HMC system to move while also increases the possibility of bouncing. At the same time, an increase in the value of $J$ lets us sample multiple points from the same ellipse without a significant increase in computation time.  

From the experiments, we observed a large disparity between the runtimes of both algorithms. As seen in the Table 1, Exact HMC performs much faster than the fastest Analytic ESS.


```{r, fig.cap="Table 1: Average computation time per sample for both models under different settings for 10-dimensional TMG under the constraints $-1 \\leq x_i \\leq 1, i=1,...,10$ using CPU. For HMC, T is the maximum travel time. For ESS, J is the number of points sampled in each iteration. The computation time for Exact HMC is overall much faster than Analytic ESS."}
# Computation time / iteration (s):
t_HMC_pi2 = readRDS("t_EHMC_HD_pi2.rds")
t_HMC_3pi4 = readRDS("t_EHMC_HD_3pi4.rds")

t_ESS_1 = readRDS("t_EESS_HD_J1.rds")
t_ESS_5 = readRDS("t_EESS_HD_J5.rds")
t_ESS_10 = readRDS("t_EESS_HD_J10.rds")

means = c(mean(t_HMC_pi2), mean(t_HMC_3pi4), mean(t_ESS_1), mean(t_ESS_5)/5, mean(t_ESS_10)/10)
M = matrix(means, ncol = 1)
M = cbind(c("HMC(T=pi/2)", "HMC(T=3*pi/4)", 
                "ESS(J=1)", "ESS(J=5)", "ESS(J=10)"),
          round(M, 7))
colnames(M) = c("Model", "Avg. computation time per sample (second)")

kable(M)


```

# 4. Discussion    

We have compared 



\newpage